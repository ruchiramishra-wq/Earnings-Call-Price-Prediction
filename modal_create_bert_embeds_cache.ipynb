{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing the Python Script\n",
        "\n",
        "This script was run on modal to generate the BERT embeddings of the transcripts in the dataset. We used modal to access GPU compute. On a L4 8-core GPU with 16GB memory, the runtime was about 5mins.\n",
        "\n",
        "Upload the prepared input csv file (which is the dataframe generated by prepare_earnings_data()) to the modal notebook root directory. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "raw_data = pd.read_csv(\"input_data_modal.csv\")\n",
        "CACHE_DIR = \"cache/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def test_train_split_by_date(df, date_col=\"adjusted_date\", train_frac=0.75, val_frac=0.12):\n",
        "    \"\"\"\n",
        "    Splits the DataFrame into train, validation, and test sets based on unique dates.\n",
        "    \n",
        "    Parameters:\n",
        "    - df: pandas DataFrame containing the data.\n",
        "    - date_col: str, name of the column containing date information.\n",
        "    - train_frac: float, fraction of data to be used for training.\n",
        "    - val_frac: float, fraction of data to be used for validation.\n",
        "    \n",
        "    Returns:\n",
        "    - train_df: DataFrame for training set.\n",
        "    - val_df: DataFrame for validation set.\n",
        "    - test_df: DataFrame for test set.\n",
        "    \"\"\"\n",
        "    # Ensure the DataFrame is sorted by date\n",
        "    df = df.sort_values(date_col).reset_index(drop=True)\n",
        "    \n",
        "    # Get unique sorted dates\n",
        "    dates = np.array(sorted(df[date_col].unique()))\n",
        "    n_dates = len(dates)\n",
        "    \n",
        "    # Calculate split indices\n",
        "    train_end = int(train_frac * n_dates)\n",
        "    val_end   = int((train_frac + val_frac) * n_dates)\n",
        "    \n",
        "    # Split dates\n",
        "    train_dates = dates[:train_end]\n",
        "    val_dates   = dates[train_end:val_end]\n",
        "    test_dates  = dates[val_end:]\n",
        "    \n",
        "    # Create DataFrames for each set\n",
        "    train_df = df[df[date_col].isin(train_dates)].reset_index(drop=True)\n",
        "    val_df   = df[df[date_col].isin(val_dates)].reset_index(drop=True)\n",
        "    test_df  = df[df[date_col].isin(test_dates)].reset_index(drop=True)\n",
        "\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def scale_features(train_df, val_df, test_df, feature_cols):\n",
        "    \"\"\"\n",
        "    Scales the specified feature columns to have mean 0 and standard deviation 1.\n",
        "    \n",
        "    Parameters:\n",
        "    - train_df: DataFrame for training set.\n",
        "    - val_df: DataFrame for validation set.\n",
        "    - test_df: DataFrame for test set.\n",
        "    - feature_cols: list of str, names of the columns to be scaled.\n",
        "    \n",
        "    Returns:\n",
        "    - features_train: numpy array of scaled features for training set.\n",
        "    - features_val: numpy array of scaled features for validation set.\n",
        "    - features_test: numpy array of scaled features for test set.\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    # Fit scaler on training data and transform\n",
        "    features_train = scaler.fit_transform(train_df[feature_cols])\n",
        "    \n",
        "    # Transform validation and test data\n",
        "    features_val   = scaler.transform(val_df[feature_cols])\n",
        "    features_test  = scaler.transform(test_df[feature_cols])\n",
        "    \n",
        "    return features_train, features_val, features_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os, glob, hashlib, shutil\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "def setup_finbert(model_name = \"yiyanghkust/finbert-tone\"):\n",
        "    \"\"\"\n",
        "    Sets up the FinBERT model and tokenizer for embedding financial text.\n",
        "\n",
        "    Returns:\n",
        "        model: The FinBERT model.\n",
        "        tokenizer: The FinBERT tokenizer.\n",
        "        device: torch.device\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    encoder.to(device)\n",
        "    encoder.eval()\n",
        "\n",
        "    for param in encoder.parameters():  # freeze FinBERT weights\n",
        "        param.requires_grad = False\n",
        "\n",
        "    return encoder, tokenizer, device\n",
        "\n",
        "\n",
        "def chunks(text, tokenizer, max_tokens=512, overlap=50):\n",
        "    \"\"\"\n",
        "    Splits the input text into chunks of tokens with specified maximum length (512) and overlap.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be chunked.\n",
        "        tokenizer: The tokenizer to convert text to token IDs.\n",
        "        max_tokens (int): Maximum number of tokens per chunk.\n",
        "        overlap (int): Number of overlapping tokens between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        List of chunks, where each chunk is a list of token IDs.        \n",
        "    \"\"\"\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=False,\n",
        "        truncation=False,\n",
        "        return_attention_mask=False\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    out = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        out.append(tokens[start:start + max_tokens])\n",
        "        start += max_tokens - overlap\n",
        "    return out\n",
        "\n",
        "\n",
        "def chunk_to_vector(chunk_id_list, encoder, tokenizer, device, batch_size=16):\n",
        "    \"\"\"\n",
        "    Takes in a list of chunks (each chunk is a list of token IDs), uses FinBERT to compute\n",
        "    CLS vector for each chunk.\n",
        "\n",
        "    Args:\n",
        "        chunk_id_list: List of chunks, where each chunk is a list of token IDs.\n",
        "        encoder: FinBERT model.\n",
        "        tokenizer: FinBERT tokenizer.\n",
        "        device: cpu or gpu device.\n",
        "        batch_size: Number of chunks to process in a batch.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor of shape (num_chunks, hidden_dim)\n",
        "    \"\"\"\n",
        "    vecs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # process in batches and prepare inputs by padding/truncating\n",
        "        for i in range(0, len(chunk_id_list), batch_size):\n",
        "            batch = chunk_id_list[i:i + batch_size]\n",
        "\n",
        "            inputs = [\n",
        "                tokenizer.prepare_for_model(\n",
        "                    ch,\n",
        "                    add_special_tokens=True,\n",
        "                    max_length=512,\n",
        "                    truncation=True,\n",
        "                    return_attention_mask=True\n",
        "                )\n",
        "                for ch in batch\n",
        "            ]\n",
        "\n",
        "            enc = tokenizer.pad(\n",
        "                inputs,\n",
        "                padding=\"max_length\",\n",
        "                max_length=512,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # ensure batch dimension\n",
        "            if enc[\"input_ids\"].dim() == 1:\n",
        "                enc[\"input_ids\"] = enc[\"input_ids\"].unsqueeze(0)\n",
        "            if enc[\"attention_mask\"].dim() == 1:\n",
        "                enc[\"attention_mask\"] = enc[\"attention_mask\"].unsqueeze(0)\n",
        "            if \"token_type_ids\" in enc and enc[\"token_type_ids\"].dim() == 1:\n",
        "                enc[\"token_type_ids\"] = enc[\"token_type_ids\"].unsqueeze(0)\n",
        "\n",
        "            enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "            out = encoder(**enc).last_hidden_state   # (B,512,768)\n",
        "            vec = out[:, 0, :]                       # (B,768) CLS embedding\n",
        "            vecs.append(vec)\n",
        "\n",
        "    vec = torch.cat(vecs, dim=0)\n",
        "    return vec  # (C,768)\n",
        "\n",
        "\n",
        "def transcript_id(text):\n",
        "    \"\"\"\n",
        "    Generates a unique identifier for a given transcript using its MD5 hash.\n",
        "    \"\"\"\n",
        "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "def build_cache(data, cache_dir, encoder, tokenizer, device, overlap=0):\n",
        "    \"\"\"\n",
        "    Takes in a dataset of transcripts, labels, and financial features, computes FinBERT\n",
        "    embeddings, and caches to disk.\n",
        "\n",
        "    Args:\n",
        "        data: List of tuples (transcript, label, fin_features).\n",
        "        cache_dir: Directory to store cached embeddings.\n",
        "        encoder: FinBERT model.\n",
        "        tokenizer: FinBERT tokenizer.\n",
        "        device: cpu or gpu device.\n",
        "        overlap: Number of overlapping tokens between consecutive chunks.\n",
        "\n",
        "    \"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for i, (transcript, y, fin_features) in enumerate(data):\n",
        "        cid = transcript_id(transcript)\n",
        "        path = os.path.join(cache_dir, f\"{cid}.pt\")\n",
        "        if os.path.exists(path):\n",
        "            continue\n",
        "\n",
        "        chunk_id_list = chunks(transcript, tokenizer, overlap=overlap)\n",
        "        Z = chunk_to_vector(chunk_id_list, encoder, tokenizer, device, batch_size=8)\n",
        "\n",
        "        f = torch.tensor(fin_features, dtype=torch.float16)\n",
        "        torch.save(\n",
        "            {\"Z\": Z.to(torch.float16), \"fin_features\": f, \"y\": int(y)},\n",
        "            path\n",
        "        )\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"[{i+1}/{len(data)}] cached | files={len(glob.glob(cache_dir+'/*.pt'))}\")\n",
        "\n",
        "    print(f\"Cached {len(data)} transcripts → {cache_dir}\")\n",
        "\n",
        "\n",
        "def zip_cache(cache_dir, zip_path):\n",
        "    \"\"\"\n",
        "    Zips the entire cache directory into a single .zip file.\n",
        "    \"\"\"\n",
        "    assert os.path.exists(cache_dir), f\"{cache_dir} does not exist\"\n",
        "    shutil.make_archive(\n",
        "        base_name=zip_path.replace(\".zip\", \"\"),\n",
        "        format=\"zip\",\n",
        "        root_dir=cache_dir\n",
        "    )\n",
        "    print(f\"Created zip file: {zip_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def create_finbert_cache(raw_data,cache_dir=CACHE_DIR,return_days=1,overlap=50):\n",
        "    \"\"\"\n",
        "    High-level function to create FinBERT cache from raw data.\n",
        "    First split data by date into train/val/test, scale financial features, zip them and then input to create finbert cache.\n",
        "\n",
        "    Args:\n",
        "        data: List of tuples (transcript, label, fin_features).\n",
        "        cache_dir: Directory to store cached embeddings.\n",
        "        overlap: Number of overlapping tokens between consecutive chunks.\n",
        "    \"\"\"\n",
        "    encoder, tokenizer, device = setup_finbert()\n",
        "\n",
        "    train_data, val_data, test_data = test_train_split_by_date(raw_data)\n",
        "    train_features,val_features, test_features = scale_features(train_data, val_data, test_data,[\"abvol_20d\", \"abcallday_r1\", \"abcallday_r5\", \"abcallday_r20\"])\n",
        "    train_transcripts,val_transcripts,test_transcripts=train_data[\"transcript\"].tolist(),val_data[\"transcript\"].tolist(),test_data[\"transcript\"].tolist()\n",
        "    \n",
        "    if return_days==1:\n",
        "        y_train_1d,y_val_1d,y_tet_1d=train_data[\"r1d_direction\"],val_data[\"r1d_direction\"],test_data[\"r1d_direction\"]\n",
        "    else:\n",
        "        y_train_1d,y_val_1d,y_tet_1d=train_data[\"r5d_direction\"],val_data[\"r5d_direction\"],test_data[\"r5d_direction\"]\n",
        "    \n",
        "    train_data, val_data, test_data = list(zip(train_transcripts, y_train_1d, train_features)), list(zip(val_transcripts, y_val_1d, val_features)), list(zip(test_transcripts, y_tet_1d, test_features))\n",
        "    \n",
        "    for data, split in zip([train_data, val_data, test_data], [\"train\", \"val\", \"test\"]):\n",
        "        split_cache_dir = os.path.join(cache_dir, split,str(return_days))\n",
        "        zip_path=os.path.join(cache_dir, f\"cache_{split}_{return_days}\"+\".zip\")    \n",
        "        build_cache(data, split_cache_dir, encoder, tokenizer, device, overlap=overlap)\n",
        "        zip_cache(split_cache_dir, zip_path)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     raw_data = prepare_earnings_data()\n",
        "#     create_finbert_cache(raw_data,cache_dir=CACHE_DIR,return_days=1,overlap=50)\n",
        "#     create_finbert_cache(raw_data,cache_dir=CACHE_DIR,return_days=5,overlap=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfde6ad4a30b43009d4b9955e2dd3aed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbadc38c516648c596f15e71a1d09ffc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c772a72b5bdd4450b304bcacaa7c75b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a7fce85270f4dfd8c8ccc14313f5b63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/439M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50/2316] cached | files=43\n",
            "[100/2316] cached | files=90\n",
            "[150/2316] cached | files=138\n",
            "[200/2316] cached | files=182\n",
            "[250/2316] cached | files=209\n",
            "[300/2316] cached | files=257\n",
            "[400/2316] cached | files=313\n",
            "[450/2316] cached | files=355\n",
            "[550/2316] cached | files=409\n",
            "[650/2316] cached | files=456\n",
            "[700/2316] cached | files=504\n",
            "[750/2316] cached | files=554\n",
            "[800/2316] cached | files=600\n",
            "[900/2316] cached | files=693\n",
            "[1000/2316] cached | files=777\n",
            "[1050/2316] cached | files=824\n",
            "[1100/2316] cached | files=866\n",
            "[1150/2316] cached | files=911\n",
            "[1200/2316] cached | files=957\n",
            "[1250/2316] cached | files=1004\n",
            "[1350/2316] cached | files=1099\n",
            "[1400/2316] cached | files=1146\n",
            "[1450/2316] cached | files=1190\n",
            "[1500/2316] cached | files=1238\n",
            "[1550/2316] cached | files=1283\n",
            "[1600/2316] cached | files=1329\n",
            "[1650/2316] cached | files=1366\n",
            "[1700/2316] cached | files=1412\n",
            "[1750/2316] cached | files=1460\n",
            "[1800/2316] cached | files=1508\n",
            "[1850/2316] cached | files=1557\n",
            "[1950/2316] cached | files=1641\n",
            "[2000/2316] cached | files=1691\n",
            "[2050/2316] cached | files=1740\n",
            "[2100/2316] cached | files=1787\n",
            "[2150/2316] cached | files=1835\n",
            "[2200/2316] cached | files=1884\n",
            "[2250/2316] cached | files=1932\n",
            "[2300/2316] cached | files=1978\n",
            "Cached 2316 transcripts → cache/train/1\n",
            "Created zip file: cache/cache_train_1.zip\n",
            "[50/515] cached | files=47\n",
            "[100/515] cached | files=94\n",
            "[150/515] cached | files=143\n",
            "[200/515] cached | files=193\n",
            "[250/515] cached | files=242\n",
            "[300/515] cached | files=291\n",
            "[350/515] cached | files=340\n",
            "[400/515] cached | files=389\n",
            "[450/515] cached | files=436\n",
            "[500/515] cached | files=486\n",
            "Cached 515 transcripts → cache/val/1\n",
            "Created zip file: cache/cache_val_1.zip\n",
            "[50/312] cached | files=48\n",
            "[100/312] cached | files=95\n",
            "[150/312] cached | files=143\n",
            "[200/312] cached | files=190\n",
            "[250/312] cached | files=239\n",
            "[300/312] cached | files=286\n",
            "Cached 312 transcripts → cache/test/1\n",
            "Created zip file: cache/cache_test_1.zip\n"
          ]
        }
      ],
      "source": [
        "create_finbert_cache(raw_data,cache_dir=CACHE_DIR,return_days=1,overlap=50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
