{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into test-train-split\n",
    "### Split the data based on time stamps into train, validation and test data making sure no information is leaked into test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use cleaned data from input_data.py\n",
    "model_input = pd.read_csv(\"C:/Users/ruchi/OneDrive/Ruchira-MAIN/Work/Stats_ML/model_input_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique dates: 511\n",
      "Rows: 2823 319 402\n",
      "Train range: 2019-04-17 → 2022-02-10\n",
      "Val range:   2022-02-11 → 2022-07-14\n",
      "Test range:  2022-07-15 → 2022-12-01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = model_input.copy()\n",
    "\n",
    "df = df.sort_values(\"adjusted_date\").reset_index(drop=True)  # Sort by date\n",
    "dates = np.array(sorted(df[\"adjusted_date\"].unique()))\n",
    "n_dates = len(dates)\n",
    "\n",
    "train_end = int(0.75 * n_dates) # 75%\n",
    "val_end   = int(0.87 * n_dates)  # 75% + 12%\n",
    "\n",
    "train_dates = dates[:train_end]\n",
    "val_dates   = dates[train_end:val_end]\n",
    "test_dates  = dates[val_end:]\n",
    "\n",
    "train_df = df[df[\"adjusted_date\"].isin(train_dates)].reset_index(drop=True)\n",
    "val_df   = df[df[\"adjusted_date\"].isin(val_dates)].reset_index(drop=True)\n",
    "test_df  = df[df[\"adjusted_date\"].isin(test_dates)].reset_index(drop=True)\n",
    "\n",
    "print(\"Unique dates:\", n_dates)\n",
    "print(\"Rows:\", len(train_df), len(val_df), len(test_df))        #ensure dates are split correctly with no data leakage\n",
    "print(\"Train range:\", train_dates[0], \"→\", train_dates[-1])\n",
    "print(\"Val range:  \", val_dates[0], \"→\", val_dates[-1])\n",
    "print(\"Test range: \", test_dates[0], \"→\", test_dates[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "y_train_1d = train_df[\"r1d_direction\"]\n",
    "y_val_1d   = val_df[\"r1d_direction\"]\n",
    "y_test_1d  = test_df[\"r1d_direction\"]\n",
    "\n",
    "y_train_5d = train_df[\"r5d_direction\"]\n",
    "y_val_5d   = val_df[\"r5d_direction\"]\n",
    "y_test_5d  = test_df[\"r5d_direction\"]\n",
    "\n",
    "#Scale features to mean 0, std 1\n",
    "features_base_train = scaler.fit_transform(train_df[[\"abvol_20d\", \"abcallday_r1\", \"abcallday_r5\", \"abcallday_r20\"]])\n",
    "features_base_val   = scaler.transform(val_df[[\"abvol_20d\", \"abcallday_r1\", \"abcallday_r5\", \"abcallday_r20\"]])\n",
    "features_base_test  = scaler.transform(test_df[[\"abvol_20d\", \"abcallday_r1\", \"abcallday_r5\", \"abcallday_r20\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model-0 (Random)\n",
    "### For ROC-AUC evaluation, we use a random baseline that assigns each transcript a score sampled uniformly from U(0,1). Since AUC is threshold-independent, no calibration is required.\n",
    "\n",
    "### For accuracy-based comparison, we additionally construct a random classifier that matches the empirical positive class rate observed in the training data. This ensures that the random baseline respects class imbalance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.49273822174991144)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_5d_train=y_train_5d.sum()/len(y_train_5d)\n",
    "p_5d_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred_random_5d= rng.uniform(size=len(y_test_5d)) #generates probabilities randomly\n",
    "y_pred_random_5d = (p_pred_random_5d >= 1-p_5d_train).astype(int) #converts probabilities to class labels based on 1-p_5d_train threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Model 5d - Accuracy: 0.5248756218905473\n",
      "Random Model 5d - AUC: 0.531331921758419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix\n",
    "accuracy_random_5d = accuracy_score(y_test_5d, y_pred_random_5d)\n",
    "auc_random_5d=roc_auc_score(y_test_5d, p_pred_random_5d)\n",
    "\n",
    "print(\"Random Model 5d - Accuracy:\", accuracy_random_5d)\n",
    "print(\"Random Model 5d - AUC:\", auc_random_5d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a sanity check, we evaluate a random classifier that predicts post-call return direction by sampling labels according to the empirical class distribution in the training set. This model achieves an AUC close to 0.53 on test data reflecting mild class imbalance rather than genuine predictive signal. Because the model does not condition on any features, this model can serve as a lower bound for our other models below. We should expect to get a better AUC if the features have predictive power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline-1 Finance Data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01: Coefficients=[[ 0.02794645  0.08204019  0.0156877  -0.22099879]]\n",
      "C=0.01: Validation AUC=0.479801559177888\n",
      "C=0.1: Coefficients=[[ 0.04263922  0.09583871  0.0280265  -0.26678343]]\n",
      "C=0.1: Validation AUC=0.4752736435939837\n",
      "C=1: Coefficients=[[ 0.04464331  0.09744065  0.0297679  -0.27265655]]\n",
      "C=1: Validation AUC=0.47495865816206007\n",
      "C=10: Coefficients=[[ 0.04485053  0.09760275  0.02995001 -0.27326128]]\n",
      "C=10: Validation AUC=0.47484053862508857\n",
      "C=100: Coefficients=[[ 0.04487132  0.09761898  0.0299683  -0.27332193]]\n",
      "C=100: Validation AUC=0.47484053862508857\n"
     ]
    }
   ],
   "source": [
    "C0=[0.01, 0.1, 1, 10, 100]\n",
    "for C in C0:\n",
    "    finance_model_5d = LogisticRegression(penalty='l2',C=C)\n",
    "    finance_model_5d.fit(features_base_train, y_train_5d)\n",
    "    val_probs_5d = finance_model_5d.predict_proba(features_base_val)[:, 1]\n",
    "    print(f\"C={C}: Coefficients={finance_model_5d.coef_}\")\n",
    "    val_auc_5d = roc_auc_score(y_val_5d, val_probs_5d)\n",
    "    print(f\"C={C}: Validation AUC={val_auc_5d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC (C=0.01): 0.480893325267191\n"
     ]
    }
   ],
   "source": [
    "finance_model_5d = LogisticRegression(penalty='l2',C=0.01)\n",
    "finance_model_5d.fit(features_base_train, y_train_5d)\n",
    "test_probs_5d = finance_model_5d.predict_proba(features_base_test)[:, 1]\n",
    "test_auc_5d = roc_auc_score(y_test_5d, test_probs_5d)\n",
    "print(f\"Test AUC (C=0.01): {test_auc_5d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We train a logistic regression model using pre-call market information, including abnormal volatility and abnormal returns prior to the earnings call. After standardizing features, the model exhibits weak anti-predictive behavior for post-call return direction, with validation AUC stabilizing around 0.47–0.48 across regularization strengths and small, stable coefficients. This pattern is consistent with short-horizon mean reversion around earnings events and indicates that simple linear market-based features alone provide limited predictive power for post-call drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline-2 TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text= train_df[\"transcript\"]\n",
    "X_val_text= val_df[\"transcript\"]\n",
    "X_test_text= test_df[\"transcript\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning function for earnings call transcripts \n",
    "import re\n",
    "\n",
    "FOOTER_MARKERS = [\n",
    "    r\"Transcript powered by\",\n",
    "    r\"This article is a transcript\",\n",
    "    r\"The Motley Fool\",\n",
    "    r\"Terms and Conditions\",\n",
    "    r\"Obligatory Capitalized Disclaimers\",\n",
    "]\n",
    "\n",
    "HONORIFIC_NAME_PATTERN = r\"\\b(Mr|Ms|Mrs)\\.?\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,2}\"\n",
    "\n",
    "def clean_transcript(text: str, keep_section=\"prepared\") -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    t = text.replace(\"\\r\\n\", \"\\n\")\n",
    "\n",
    "    # 1) Cut off provider/legal footer\n",
    "    footer_pat = r\"(?i)(\" + \"|\".join(FOOTER_MARKERS) + r\").*\"\n",
    "    t = re.sub(footer_pat, \" \", t, flags=re.DOTALL)\n",
    "\n",
    "    # 2) Keep only Prepared Remarks\n",
    "    if keep_section == \"prepared\":\n",
    "        m = re.search(r\"(?is)Prepared Remarks:\\s*(.*?)(?:Questions and Answers:|$)\", t)\n",
    "        if m:\n",
    "            t = m.group(1)\n",
    "\n",
    "    # 3) Remove speaker header lines\n",
    "    t = re.sub(r\"(?m)^[A-Z][A-Za-z\\.\\-\\s]{1,80}\\s+--\\s+.*$\", \" \", t)\n",
    "\n",
    "    # 4) Remove operator / queue scaffolding\n",
    "    t = re.sub(r\"(?im)^operator.*$\", \" \", t)\n",
    "    t = re.sub(r\"(?im)^our (?:next|first|last) question.*$\", \" \", t)\n",
    "    t = re.sub(r\"(?im)^your line is open.*$\", \" \", t)\n",
    "    t = re.sub(r\"(?im)^\\(operator instructions\\).*$\", \" \", t)\n",
    "\n",
    "    # 5) Remove honorific + names (Mr./Ms./Mrs.)\n",
    "    t = re.sub(HONORIFIC_NAME_PATTERN, \" \", t)\n",
    "\n",
    "    # 6) Normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Welcome and thank you for standing by. At this time, all participants are in a listen-only mode. Today's conference is being recorded. If you have any objections you may disconnect at this time. Now I will turn the meeting over to with IBM. Ma'am, you may begin. Thank you. This is Patricia Murphy, Vice President of Investor Relations for IBM. And I want to welcome you to our First Quarter 2019 Earnings Presentation. I'm here with Jim Kavanaugh, IBM's, Senior Vice President and Chief Financial Officer. We'll post today's prepared remarks on the IBM Investor website within a couple of hours and a replay will be available by this time tomorrow. Some comments made in this presentation may be considered forward-looking under the Private Securities Litigation Reform Act of 1995. Those statements involve factors that could cause our actual results to differ materially. Additional information about these factors is included in the Company's SEC filings. Our presentation also includes non-GAAP measures to provide additional information to investors. We've provided reconciliation charts at the end of the presentation and in the 8-K submitted to the SEC. Before turning the call over to Jim, I want to remind you we recently made changes to our management system and our organizational structure. Our segment reporting for 2019 has been updated to reflect this business structure. We provided two years of historical financial information by quarter on these segments a couple of weeks ago. And this can be found on our Investor website. And today we'll be discussing our first quarter results in this new segment structure. So with that, I'll turn the call over to Jim. Thanks, Patricia, and thanks to all of you for joining us. In the first quarter, we delivered $18.2 billion of revenue with significant operating leverage. We delivered $2.2 billion of operating pre-tax income and $2.25 of operating earnings per share. And we've now generated over $12 billion of free cash flow over the last year, with realization well over 100%. We had strong performance and offerings that help clients with their digital transformations and Journeys to cloud. At the same time, we continue to take actions to optimize our portfolio while investing to lead in the emerging high value areas of the IT industry. You saw this play out in our results. Our cloud growth accelerated to 12% at constant currency. Our cloud and cognitive software was up 2% and our consulting revenue was up 9% also both at constant currency. We had significant margin expansion with operating gross margin up 90 basis points, driven by both services segments and we had solid free cash flow. Improving margin has been a focus for us. And our performance this quarter is the result of actions we've been taking, not only our focus on higher value and portfolio optimization, but also driving productivity and operational efficiency, especially in our services business. With this start to the year, we are maintaining our full year expectations for operating earnings per share and free cash flow. In the first quarter, our revenue was down less than 1 point year-to-year at constant currency, slightly better than our fourth quarter performance. Our reported revenue as expected includes a significant currency headwind, as always, our focus on constant currency performance. From a geographic perspective, our year-to-year revenue growth in the developed markets improved a couple of points sequentially, consistent with the expectations we discussed last quarter. That said, we had weaker performance in the emerging markets in Asia Pacific, which impacted our overall revenue performance. Looking at our results by segment, we had continued revenue growth in global business services and in our cloud and cognitive software. In GBS, as I said, we had another quarter of strong growth in consulting, as we help clients with their digital reinventions. And we again, expanded our margins in GBS. In our software segment, growth was driven by our hybrid cloud offerings, security and solution areas like supply chain and Watson Health. In Global Technology Services, we're continuing to help our clients implement and manage hybrid multi cloud environments. This is evidenced in the increasing share of our backlog, which is now cloud. At the same time, consistent with our high-value focus, we're continuing to take actions to optimize our GTS performance by exiting lower value content. While this contributes to lower GTS revenue, we had higher profit, margin and cash contribution. Our systems revenue declined, reflecting the IBM Z product cycle dynamics and weaker performance in storage. Across our segments, clients continue to be focused on solutions that deliver innovation and growth. Though as we mentioned last quarter, we're seeing an increasing bias toward engagements that provide productivity and predictability of spend. And so our results this quarter reflect our ability to deliver both innovation and productivity, helping our clients transition their business models to hybrid cloud. Our cloud revenue growth in the first quarter accelerated to 12% with our as-a-service offerings, up 15%. With this, our cloud revenue has grown to $19.5 billion over the last year. Over the last several months, we've talked about the next chapter of cloud, which focuses on shifting mission critical work to the cloud and optimizing everything from supply chains, the core banking systems. To address this opportunity, enterprises need to be able to move and manage data, services and workflows across multiple clouds and on-prem. And they need to be able to address security concerns, data protection and protocols, availability and cloud management. This requires a hybrid multi-cloud open approach. And so we have been reshaping our business to address this opportunity, investing heavily to build capabilities across our business, like IBM Cloud, IBM Cloud private and IBM Cloud private for data. The IBM multi-cloud manager, cloud garages, cloud migration services and cloud optimized systems. These are the innovations that are driving our $19.5 billion of cloud revenue. In the first quarter, we introduced additional capabilities that will accelerate hybrid cloud adoption, including Watson Anywhere which makes IBM Watson available on-premises, as well as on any private or public cloud. And IBM cloud integration platform, which provides a standard way to integrate services and applications across multiple cloud environments. More broadly, we have built a framework of offerings to facilitate our clients' journey to the cloud. It is designed to help our clients across the four key stages of their cloud transformation journey: advise, move, build and manage. These offerings span our cloud and cognitive software, global business services and global technology services, leveraging the integrated value of IBM. And so we have a strong foundation for the addition of Red Hat. Together, we will be ideally positioned to help our clients shift their business applications to hybrid cloud, while addressing the issues I just mentioned around portability, management consistency, security, remaining open which avoids vendor lock in. This will not only enhance the growth at Red Hat's business after closing, but lift all of IBM. As we sell more of our data in AI Software on containers across multiple platforms and more of our services from app modernization to multi-cloud managed services. At IBM, we're investing and building capabilities to be ready to drive these synergies. We're moving through the regulatory process and continue to expect to close in the second half of 2019. Before getting into the financial metrics, I want to lay out the contributors to our year-to-year operating earnings per share performance, especially because there are couple of larger items in last year's results that impact the dynamics. In fact , these larger items contributed a $0.32 benefit to last year's earnings per share, which of course creates a headwind to this year's growth. And so looking at the drivers, as I said, our revenue was down less than 1% at constant currency. But with the stronger dollar, revenue was down 4.7%. At constant margin, revenue was a headwind of profit and earnings-per-share growth. Last year, we took pre-tax charges associated with the actions to realign our skills to key opportunities and better position our systems' cost structure. These, together with the benefit of the actions and our ongoing operating efficiencies resulted in strong pre-tax income growth and pre-tax margin expansion. Last year, we also had a large discrete tax benefit associated with an audit settlement. With a much smaller discrete benefit this year, tax was a significant headwind to our net income. And finally, a lower share count contributed to growth. Putting this all together, we had solid operating leverage and margin expansion, offset by a $0.32 impact of last year's significant items, resulting in an operating EPS of $2.25. So now getting into profit and margin metrics, we continue to drive operating leverage, expanding both gross and pre-tax margins. Our operating gross margin was up 90 basis points. This was driven by strong performance in both services businesses together up 160 basis points. We also had a year-to-year benefit from the charge we took last year in systems, which was offset by the impact of the IBM Z product cycle. Our operating expense was better 11%, which resulted in a 2 point benefit in our expense to revenue ratio. Overall, we've been driving productivity in our business, including implementing new ways of working and leveraging automation and infusing AI into our processes. This drives operating leverage and provides flexibility to increase investment in areas like hybrid cloud, AI and blockchain. But we have a few other drivers of our expense performance this quarter, including currency and lower workforce rebalancing charges, mitigated by a lower level of IP income. Regarding currency, while the stronger dollar hurts the top line, it generally helps expense, due both to translation and the benefit of hedging contracts. In the first quarter, currency helped our year-to-year expense by nearly 6 points. Much of this was reflected in other income and expense. In fact, the $200 million year-to-year change in other income and expense was entirely due to hedging benefits. Remember these hedging gains mitigate the currency impacts throughout the P&L. Expense also includes a year-to-year reduction of over $500 million for workforce rebalancing, driven by last year's charge. And finally, within expense we absorbed the lower level of IP income as it hurts our PTI growth by over $200 million. Putting this expense performance together with our gross margin expansion, pre-tax margin was up over 300 basis points. Our operating tax rate was 10% including discretes. This is right in line with our all-in first quarter expectation of 10% to 11% we provided in January. And as I said earlier, this was a significant headwind to our net income growth year-to-year. Looking at our cash metrics, we generated $1.7 billion of free cash flow in the quarter, which is up about $350 million over last year. There is a lot of seasonality in our cash generation and so looking over the last 12 months, we generated over $12 billion of free cash flow, that's a 114% of our GAAP net income normalized. I'll touch on the cash drivers and uses of cash a little later. And so now before getting into the segment performance, I want to spend a minute on an overview of our 2019 segment structure. As Patricia mentioned, we shared historical information on this new structure a couple weeks ago. As our clients become digital enterprises, they need tighter integration between hybrid cloud and their data and AI platforms to unlock value. And so we recently made changes to our management system, to more effectively address our clients' evolving needs and in preparation for the acquisition of Red Hat. The changes also better align our portfolio to the market and to underlying business models. The business changes resulted in three adjustments to our segment structure for 2019. First, we brought our cloud and cognitive software together in one segment. Second, we combined our security services with security software, consistent with the way we are running that integrated business. And then finally, we moved the results for the businesses we're divesting to the other category, to provide better transparency to the ongoing operational performance of our software and GBS segments. This includes the pending sale of our collaboration and on-prem marketing and commerce software to HCL. The pending sale of the balance of our marketing and commerce software to Centerbridge and the just completed sale of our Seterus mortgage servicing business. And so looking at our new segments, we created the cloud and cognitive software segment, bringing software platforms and solutions into one segment. Within this segment, we'll report cloud and data platforms, which brings together software for hybrid cloud management with data and AI platforms. Cognitive applications includes vertical and domain specific solutions, that are built on cloud and data platforms. These offerings are increasingly being infused with AI. And then transaction processing platforms, includes the middleware and database software that supports our clients' mission critical workloads running on z/OS as well as storage software. Looking at our services segments, the scope of global business services segment overall is unchanged other than moving the divested mortgage servicing business to other. Global technology services is consistent with the services component of technology services and cloud platforms excluding security services. And then finally, our systems and global financing segments are also unchanged. So now let me get into the segment results, starting with our cloud and cognitive software segment, where revenue grew 2%. Our clients' journey to cloud in AI is now turning to more mission-critical workloads. As I just mentioned, linking the data, AI and applications together with hybrid cloud in a secure way is critical for any successful digital reinvention. We are uniquely positioned to do this with our comprehensive cloud and data offerings, coupled with a deep understanding of our clients' workflows and security needs. Within this segment, we had good growth in cloud and data platforms and cognitive applications, while transaction processing platforms was flat. I'll break down some of the drivers behind these areas. Our cloud and data platforms grew 2%. We delivered growth this quarter by helping clients build across public and private clouds with IBM cloud private, which as you know is built on Linux containers and Kubernetes. We help them modernize and integrate applications and environments with our integration and digital business automation platforms. And then collect and manage data with the hybrid data management platform, all of which grew this quarter. This need for tighter integration across hybrid cloud, data and AI are also driving traction for our IBM Cloud Private for data offering. As well as Watson Assistant and Watson OpenScale, that run on IBM cloud private for data. We see the value of bringing together the hybrid cloud and data value propositions at a European tax authority, which is using our digital business automation platform to redesign their tax processes around their Data Lake and improve the tax payer experience. In cognitive applications, revenue was up 4%. Growth was led by security as well as solution areas like health, supply chain, and weather. In security, we delivered strong double-digit growth with our integrated software and services value proposition. In particular, we continue to see good traction with our threat management software and services offerings including QRadar and Resilient. And our security intelligent operations consulting services, which detect and respond to security threats for our clients. Panasonic, for example, is leveraging QRadar and related services to strengthen its threat management posture. Panasonic is also piloting our NextGen X-Force threat management offering. In Watson Health, we had broad based growth across areas including payer, provider and government, as clients look to harness data to create actionable insights. We also had good results from our weather offerings, which grew double-digits this quarter and reached a new all-time high in the number of active users. Transaction processing platforms revenue was consistent with last year, as clients continue to commit to our platform for the longer term. Performance reflects the value we provide clients managing these vital workloads and their preference for predictability in IT spend. Turning to profit for the segment, we expanded pre-tax margin by 2 points year-to-year. This reflects the lower level of workforce rebalancing this year, mitigated by a headwind in IP income and continued investment in key strategic areas. As we look forward, essentially all of our software portfolio now runs on Linux containers orchestrated by Kubernetes. We've introduced new offerings like IBM cloud integration platform, the digital business automation platform and Watson Anywhere to further accelerate hybrid cloud adoption. And we have ongoing activities to educate all of IBM's employees on the journey to cloud, which includes Red Hat skills. All of this better prepares us for the Red Hat acquisition. Moving to global business services, we continued the momentum from last year and delivered another solid quarter. Revenue grew 4% and gross margin expanded 280 basis points. We again had strong growth in consulting, which was up 9%. As clients embark on their digital journey to a cognitive enterprise, they are turning to GBS to help them with their strategy and implementation, leveraging our deep industry expertise and innovative technology portfolio. The growth this quarter was led by digital strategy and IX, as well as consulting for cloud application migrations and our next generation enterprise application practice. Within cloud application migration, GBS cloud advisory services works with enterprises to plan and implement a clear strategy and roadmap for their hybrid cloud journey. We are doing this with Tribune Publishing as they transform from a legacy print company to a digital company, helping them determine the right environment for each of their applications and optimizing their migration to the cloud. Our next generation enterprise application offerings assist clients, as they build and implement cloud native applications. In areas such as workday, sales force and S/4 HANA. IBM is now leading the market with over 200 S/4 HANA impact assessments, over 200 implementations and more than 125 go lives. In application management, we are shifting our business to cloud-based offerings and continue to have good momentum in our cloud migration factory and cloud application development. Overall, application management revenue was flat, due to ongoing declines in the traditional application management engagements. And then, global process services had solid performance in first quarter. Revenue was up 5% with strong performance in risk and compliance, along with financial process services. Turning to GBS profit, our gross margin was 26%, which is up 280 basis points, driven by our mix to higher value offerings, the yield on our productivity and utilization initiatives and a continuing help from currency, given our global delivery mix. This enables us to make investments as we prepare for the Red Hat acquisition, such as scaling our existing Red Hat practice to enhance our journey to cloud offerings for clients leveraging Red Hat capabilities. We're also creating new offerings around advise, build, move and managed services, through industry points of view and platform plays. In global technology services, as I mentioned last quarter, we are taking actions to optimize our portfolio by exiting low value services content to increase margin, profit and cash contribution and better position the business for the longer term. GTS plays an important role in IBM's integrated value proposition, building on its deep client relationships to shift our clients to hybrid cloud. As we moved through last year, we improved GTS profit and margin, creating operating leverage. This gives us a solid base from which we can de-emphasize lower-value contracts and third-party content, enabling continued investment for Chapter II of the cloud and delivering sustained margin improvement. This is where we're focused. We saw this play out in our first quarter results, as overall revenue declined, but gross margin expanded 110 basis points, driven by the mix shift to higher value, a lift from cloud scale efficiencies and productivity improvements. So now looking at the GTS revenue by line of business. Infrastructure and cloud services was down 3%, and technology support services was down 2%. Within infrastructure and cloud services, we continue to have solid growth in cloud revenue, which was up 13%. This is driven by the backlog where cloud is now over 30% of the total services outsourcing backlog. Keep in mind most of the cloud opportunity is ahead of us. 80% of the enterprise workloads which represent mission critical work have yet to move to the cloud. As clients migrate these workloads to a hybrid multi-cloud environment, they faced increased complexity in managing their infrastructure. Because we've been running these workloads, we're better positioned to help our clients to build and manage these new environments. During the first quarter, we announced that we're moving and managing BNP Paribas and Santander, a couple of the largest banks in Europe to hybrid cloud. That's on top of companies like Lloyds, Allianz, Westpac, American Airlines and Anthem Insurance, the list goes on. That's all mission critical work starting to move and they're moving it with IBM. The portfolio actions I mentioned earlier, create flexibility to invest in additional cloud capabilities to capture this high value growing market. For example, our IBM services for multi-cloud management offering provides a single system to help enterprise simplify the management of their IT resources across multiple cloud providers, on-prem environments and private clouds. And as we prepare for the Red Hat acquisition, we are investing to build on our partnership as a services integrator for Red Hat to be a leader in hybrid multi-cloud services. In systems, revenue was down 9% this quarter with declines in IBM Z reflecting where we are in the product cycle, and in storage driven by market and competitive dynamics. That said, we had good performance in Power. This quarter, IBM Z revenue declined 38%. I'll remind you we are wrapping on strong performance from last year when we had 54% growth. We are seven quarters into the z14 cycle and the program continues to track ahead of the prior program. We had strong growth in volumes or shipped MIPS, and new workload MIPS, continue to outpace our standard MIPS. This growth is led by Linux again this quarter. And our Single Frame z14 designed specifically for cloud datacenters remains a growth driver. Power revenue grew for the sixth consecutive quarter, up 9% driven by Linux and the full rollout of our Power9 based architecture. As clients look to handle more data intensive workloads in AI, HANA and UNIX, they are turning to Power9 systems. These systems are built to handle advanced analytics and cloud environments. Both the high end entry-level offerings posted strong growth this quarter, as clients continue to adopt this new technology. Storage hardware was down 11% with declines in both the high-end and mid-range, offset by continued growth in all-flash arrays. Performance reflects declines in our high-end, which is tied to our mainframe cycle and the ongoing competitive dynamics and pricing pressures. We are continuing to introduce new innovations and functionality to differentiate in this environment as we look to manage the portfolio for the market shift to flash. Looking at systems' profit, pre-tax margin was down 1 point, driven by mix headwind due to where we are in the z14 cycle. So, now turning to cash flow, we've generated $2.3 billion of cash from operations in the quarter excluding our financing receivables. Our free cash flow of $1.7 billion is up about $350 million year-to-year. This performance results in free cash flow of $12.2 billion over the last 12 months and continued strength in our normalized free cash flow realization rate, which is 114%. Our CapEx decline reflects effective capital management, and the strategy I mentioned earlier to de-emphasize some lower value content. This reduces our capital requirements. And so free cash flow came in where we expected, and there is no change in our full year outlook of about $12 billion. Looking at uses of cash, we've returned $2.3 billion to the shareholders in the quarter, including $1.4 billion of dividends and over $900 million of gross share repurchases. That's $10.3 billion over the last 12 months. We bought back nearly 7 million shares and at the end of the quarter, we had $2.4 billion remaining in our buyback authorization. I'll remind you, we plan to suspend share repurchase in 2020 and 2021 as we pay down debt for our Red Hat acquisition to get back to our targeted leverage ratio. Looking at the balance sheet, we closed the quarter with a cash balance of over $18 billion and total debt of $50 billion. Both of these are up from December as we prepare for the acquisition of Red Hat later in the year. About 60% of our total debt is in support of our financing business. The leverage in our financing business remains at 9:1. And the credit quality of our financing receivables remained strong at 55% investment grade. That's 2 points better than a year ago. As a reminder, our financing debt will decrease throughout the year as a result of the winding down of our commercial OEM content. So to summarize, free cash flow is on track and our balance sheet reflects the strength required to support our continuing investments and return to shareholders. So let me make a few summary comments on the quarter and our view of the year before we move on to Q&A. In the first quarter, we grew in key high value segments, led by global business services and cloud and cognitive software. While our overall revenue reflects the IBM Z product cycle dynamics and our focus on de-emphasizing lower value work and services. Across IBM, our cloud growth accelerated, as we help our clients transition their business models to hybrid environments. We had significant margin expansion with gross margin up over 90 basis points. This reflects our shift to higher value, and our focus on productivity and operational efficiencies, what I'd characterize as improving fundamentals. We are continuing to prioritize our investments and announced additional actions to divest some businesses that aren't contributing to the integrated value proposition for our clients. And we're continuing our planning and preparation for the acquisition of Red Hat. With this performance, we continue to expect to deliver at least $13.90 of operating earnings per share, and about $12 billion of free cash flow. I want to remind you, what is and is not included in these expectations. And this is consistent with what we discussed last quarter. We continue to expect Red Hat to close in the second half. Because the financial implications to the year are dependent on the timing of the closing, we have not included Red Hat in the expectations. In contrast, the timing of the closing of our two remaining announced divestitures does not have a significant impact on the year. That's because we continue to expect the combination of the foregone profit, the gain on sale, the actions to address the structure and stranded cost, and the resulting benefit from these actions to have minimal impact to our profit and earnings per share for the year. And so, our guidance assumes, these divestitures Looking at the skew of earnings per share for the year, we assume we'll deliver about 22% in the second quarter, in line with the last couple of years. And then looking at the second half, we would expect the growth in EPS to be skewed to the fourth quarter. This assumes we'll close the software divestitures in the second quarter with the gain effectively offset by the foregone profit and the charges for actions to address the structure and stranded costs. In other words, we expect essentially no impact to the second quarter. Looking at free cash flow, we do expect an impact from the divestitures, as well as some pre-closing financing costs for the Red Hat acquisition. But with a solid start to the year at free cash flow, we are comfortable that we can absorb these headwinds in the full year expectation of about $12 billion. And with that, let me turn it back to Patricia for the Q&A. Thank you, Jim. Before we begin the Q&A, I'd like to mention a couple of items. First, we have supplemental charts at the end of the slide deck that provide additional information on the quarter. And second, as always, I'd ask you to refrain from multi-part question. So, operator, let's please open it up for questions.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text_cleaned=X_train_text.map(lambda x: clean_transcript(x, keep_section=\"prepared\"))\n",
    "X_val_text_cleaned=X_val_text.map(lambda x: clean_transcript(x, keep_section=\"prepared\"))\n",
    "X_test_text_cleaned=X_test_text.map(lambda x: clean_transcript(x, keep_section=\"prepared\"))\n",
    "X_train_text_cleaned.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "MONTHS = {\n",
    "    \"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\n",
    "    \"september\",\"october\",\"november\",\"december\"\n",
    "}\n",
    "\n",
    "EARNINGS_BOILERPLATE = {\n",
    "    \"quarter\",\"quarters\",\"year\",\"years\",\"fiscal\",\"calendar\", \"quarterly\",\n",
    "    \"q1\",\"q2\",\"q3\",\"q4\", \"first\", \"second\", \"third\", \"fourth\",\n",
    "    \"thank\",\"thanks\",\"appreciate\",\"welcome\",\"morning\",\"afternoon\",\"evening\",\n",
    "    \"today\",\"joining\",\"begin\",\"start\", \"call\",\"calls\",\"host\",\"operator\",\n",
    "    \"conference\",\"webcast\",\"presentation\",\"remarks\",\"prepared\",\"questions\",\"company\"\n",
    "}\n",
    "\n",
    "STOPWORDS = ENGLISH_STOP_WORDS.union(MONTHS).union(EARNINGS_BOILERPLATE)\n",
    "STOPWORDS = list(ENGLISH_STOP_WORDS.union(MONTHS).union(EARNINGS_BOILERPLATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=STOPWORDS,\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z]+\\b\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=10,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True,\n",
    "    max_features=50_000\n",
    ")\n",
    "    \n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text_cleaned) \n",
    "X_val_tfidf = vectorizer.transform(X_val_text_cleaned)\n",
    "X_test_tfidf = vectorizer.transform(X_test_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01: Validation AUC=0.5650838648712497\n",
      "C=0.01: Test AUC=0.5023694293204275\n",
      "C=0.1: Validation AUC=0.574533427828963\n",
      "C=0.1: Test AUC=0.5115950796531559\n",
      "C=1: Validation AUC=0.5922907315536656\n",
      "C=1: Test AUC=0.5175438596491228\n",
      "C=10: Validation AUC=0.5905583116780849\n",
      "C=10: Test AUC=0.5254587618471467\n",
      "C=100: Validation AUC=0.589731474919285\n",
      "C=100: Test AUC=0.5299959669288163\n"
     ]
    }
   ],
   "source": [
    "Cs=[0.01, 0.1, 1, 10, 100]\n",
    "best=None\n",
    "for C in Cs:\n",
    "    text_model_5d = LogisticRegression(penalty='l2',C=C,solver='liblinear',max_iter=2000)\n",
    "    text_model_5d.fit(X_train_tfidf, y_train_5d)\n",
    "    val_probs_5d = text_model_5d.predict_proba(X_val_tfidf)[:, 1]\n",
    "    val_auc_5d = roc_auc_score(y_val_5d, val_probs_5d)\n",
    "    test_probs_5d = text_model_5d.predict_proba(X_test_tfidf)[:, 1]\n",
    "    test_auc_5d = roc_auc_score(y_test_5d, test_probs_5d)\n",
    "\n",
    "    print(f\"C={C}: Validation AUC={val_auc_5d}\")\n",
    "    print(f\"C={C}: Test AUC={test_auc_5d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC (C=1) TF-IDF: 0.5175438596491228\n"
     ]
    }
   ],
   "source": [
    "# pick C=1 as final choice for standard regularization\n",
    "C0=1\n",
    "text_model_5d = LogisticRegression(penalty='l2',C=C0,solver='liblinear',max_iter=2000)\n",
    "text_model_5d.fit(X_train_tfidf, y_train_5d)\n",
    "test_probs_5d = text_model_5d.predict_proba(X_test_tfidf)[:, 1]\n",
    "test_auc_5d = roc_auc_score(y_test_5d, test_probs_5d)\n",
    "print(f\"Test AUC (C=1) TF-IDF: {test_auc_5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive words:\n",
      "later                          6.3358\n",
      "reflects                       4.7089\n",
      "relations good                 4.6346\n",
      "implementation                 4.5544\n",
      "tom                            4.4531\n",
      "adjust                         4.3786\n",
      "winter                         4.3456\n",
      "updated guidance               4.2747\n",
      "senior leadership              4.1812\n",
      "end markets                    4.1204\n",
      "penetrated                     4.0880\n",
      "rob                            4.0533\n",
      "pandemic related               3.9406\n",
      "freight                        3.9354\n",
      "weather                        3.8822\n",
      "presence                       3.8511\n",
      "cases                          3.8418\n",
      "reflects increase              3.7899\n",
      "good hope                      3.7811\n",
      "retained                       3.7809\n",
      "\n",
      "Top negative words:\n",
      "occurred                       -5.1083\n",
      "low                            -4.7262\n",
      "capable                        -4.6684\n",
      "covid pandemic                 -4.5993\n",
      "award                          -4.5403\n",
      "anticipated                    -4.4189\n",
      "taking                         -4.3487\n",
      "delta                          -4.2982\n",
      "west                           -4.2967\n",
      "million compared               -4.2735\n",
      "immediately                    -4.2577\n",
      "feel good                      -4.1506\n",
      "backdrop                       -4.1170\n",
      "risks factors                  -4.1109\n",
      "hurricane                      -4.0579\n",
      "factored                       -4.0262\n",
      "industries                     -4.0084\n",
      "transaction                    -3.9620\n",
      "face                           -3.9428\n",
      "shut                           -3.9386\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of words to show\n",
    "TOP_K = 20\n",
    "\n",
    "# get feature names and coefficients\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coefs = text_model_5d.coef_[0]   # shape: (num_features,)\n",
    "\n",
    "# top positive words (push prediction toward y=1)\n",
    "top_pos_idx = np.argsort(coefs)[-TOP_K:][::-1]\n",
    "top_pos = list(zip(feature_names[top_pos_idx], coefs[top_pos_idx]))\n",
    "\n",
    "# top negative words (push prediction toward y=0)\n",
    "top_neg_idx = np.argsort(coefs)[:TOP_K]\n",
    "top_neg = list(zip(feature_names[top_neg_idx], coefs[top_neg_idx]))\n",
    "\n",
    "print(\"Top positive words:\")\n",
    "for w, c in top_pos:\n",
    "    print(f\"{w:30s} {c:.4f}\")\n",
    "\n",
    "print(\"\\nTop negative words:\")\n",
    "for w, c in top_neg:\n",
    "    print(f\"{w:30s} {c:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We apply a TF-IDF representation to cleaned earnings-call prepared remarks and train a logistic regression classifier to predict post-call return direction. The text-only model achieves modest but consistent out-of-sample predictive power, with test AUC reaching approximately 0.52–0.53, outperforming both the random and finance-only baselines. This result suggests that linguistic features in earnings-call transcripts contain incremental information relevant for post-call returns. To avoid overfitting through hyperparameter selection, we fix the regularization strength at C=1, which yields stable validation performance and a test AUC of approximately 0.52."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline-3 Finance+TF-IDF Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_finance_tfidf_train = np.hstack([features_base_train, X_train_tfidf.toarray()])\n",
    "features_finance_tfidf_val   = np.hstack([features_base_val, X_val_tfidf.toarray()])\n",
    "features_finance_tfidf_test  = np.hstack([features_base_test, X_test_tfidf.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC (Finance + Text, C=1): 0.513132688041944\n"
     ]
    }
   ],
   "source": [
    "finance_text_model_5d = LogisticRegression(penalty='l2',C=1.0,solver='liblinear',max_iter=2000)\n",
    "finance_text_model_5d.fit(features_finance_tfidf_train, y_train_5d)\n",
    "test_probs_5d = finance_text_model_5d.predict_proba(features_finance_tfidf_test)[:, 1]\n",
    "test_auc_5d = roc_auc_score(y_test_5d, test_probs_5d)\n",
    "print(f\"Test AUC (Finance + Text, C=1): {test_auc_5d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We next combine TF-IDF text features with standardized pre-call financial variables in a single logistic regression model. The combined model does not improve out-of-sample performance relative to the text-only specification, achieving a test AUC of approximately 0.51. This suggests that, in a linear setting, the predictive signal contained in earnings-call language is largely orthogonal to simple market-based features, and that adding finance variables may partially dilute the text signal rather than enhance it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Across linear baselines, we find that earnings-call text contains modest but robust out-of-sample predictive information for post-call returns, while simple market-based features alone provide limited explanatory power. However, linear models may be insufficient to capture non-linear interactions, contextual sentiment, and semantic structure present in earnings-call language. In the next stage, we therefore move to more expressive text representations using pretrained transformer-based models, such as FinBERT, which are designed to capture financial sentiment and contextual meaning beyond bag-of-words approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'FinBert_Utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mFinBert_Utilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mFBU\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'FinBert_Utilities'"
     ]
    }
   ],
   "source": [
    "import FinBert_Utilities as FBU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ruchi\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\My-Applications\\\\Anaconda\\\\python313.zip',\n",
       " 'c:\\\\My-Applications\\\\Anaconda\\\\DLLs',\n",
       " 'c:\\\\My-Applications\\\\Anaconda\\\\Lib',\n",
       " 'c:\\\\My-Applications\\\\Anaconda',\n",
       " '',\n",
       " 'c:\\\\My-Applications\\\\Anaconda\\\\Lib\\\\site-packages',\n",
       " 'c:\\\\My-Applications\\\\Anaconda\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'c:\\\\My-Applications\\\\Anaconda\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'c:\\\\My-Applications\\\\Anaconda\\\\Lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
